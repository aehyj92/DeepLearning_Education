{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e50b43",
   "metadata": {},
   "source": [
    "# 신경망과 딥러닝\n",
    "> - 지능적인 기계를 만드는법에 대한 영감을 얻기 위해 뇌 구조를 살펴보게 된 것이 인공 신경망의 출발점이다.</br></br>\n",
    ">\n",
    "> - 다재 다능한 인공 신경망은 강력하고 확장성이 좋아 이미지를 분류하거나, 음성인식 서비스의 성능을 높이거나, 개인화 된 추천서비스를 제공하거나, 바둑 세계 챔피언을 이기기 위해 수백만 개의 기보를 익히고 자기 자신과 게임하는 등 아주 복잡한 대규모 머신러닝 문제를 다룰 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85e0e9",
   "metadata": {},
   "source": [
    "## Basic concept\n",
    "\n",
    "### ANN(Artificial Neural Network)의 Basic Idea** </br>\n",
    "\n",
    "![ANN](https://mblogthumb-phinf.pstatic.net/20161003_66/12kyny_1475500967723aoj5N_PNG/%B1%E2%B0%E8%C7%D0%BD%C0%28%B8%D3%BD%C5%B7%AF%B4%D7_Machine_Learning%29_%C0%CE%B0%F8%BD%C5%B0%E6%B8%C1%28Artificial_Neural_Network%29_01.png?type=w800)\n",
    "                                                               [그림1-1]\n",
    "\n",
    "\n",
    "<!-- **학습 모식도** -->\n",
    "\n",
    "<!-- ![ANN](https://mblogthumb-phinf.pstatic.net/20161003_264/12kyny_1475503510763gIa4b_PNG/%B1%E2%B0%E8%C7%D0%BD%C0%28%B8%D3%BD%C5%B7%AF%B4%D7_Machine_Learning%29_%C0%CE%B0%F8%BD%C5%B0%E6%B8%C1%28Artificial_Neural_Network%29_02.png?type=w800) -->\n",
    "**A : Neuron의 생물학적 구조 </br></br>\n",
    "B :** \n",
    ">$X_1,X_2,X_3,...,X_n$ = dendrites(수상돌기)를 통해 들어오는 전기적 신호</br></br>\n",
    "  $f(x)$ = dendrites를 통해 받아들인 신호를 중요도에 따라 가중치를 두고 integral</br></br>\n",
    "&nbsp;&nbsp; &nbsp;= $input$값에 개별적 가중치를 곱하여 모두 더해주는 함수(weighted sum)</br></br>\n",
    "\n",
    "**C : 시냅스, 다음 뉴런으로 신호를 전달하기 위한 연결고리</br></br>\n",
    "D : 이전 뉴런의 output값이 다음 뉴런의 input값으로 전달**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45899a18",
   "metadata": {},
   "source": [
    "### Perceptron</br>\n",
    "- Perceptron</br>\n",
    ">- 가장 간단한 인공 신경망 구조 중 하나로 TLU(threshold logic unit)으로 불리는 인공뉴런을 기반으로 한다.\n",
    ">- 입력과 출력이 이진값(on/off)이 아닌 어떤 숫자이고, 각각의 입력 연결은 가중치와 연관된다\n",
    ">- TLU는 입력의 weighted sum을 구하여($z = w_1x_1+w_2x_2+...+w_nx_n = x^Tw$) 계단함수(step function)를 적용하여 결과를 출력한다.\n",
    ">- $h_w(x) = step(z)$ \n",
    ">![](https://o.quizlet.com/aZka55O26J7A2H9gqoDYGQ.png)\n",
    "\n",
    "</br>\n",
    "\n",
    "- FC(Fully Connected)layer</br>\n",
    "\n",
    ">- 퍼셉트론은 층이 하나인 TLU로 구성되는데, TLU는 모든 입력에 연결이 되어있다.\n",
    ">\n",
    ">- 한 층에 있는 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때 이를 FC(Fully Connected)layer 또는 Dense layer라고 부른다.\n",
    ">\n",
    ">![](https://cdn-images-1.medium.com/max/600/1*yjy3dwRL-vmSpmUG7UNJYg@2x.png)\n",
    "\n",
    "- Digital logic gate</br>\n",
    "Digital logic gate를 이용하여 퍼셉트론의 약점을 알아보고 XOR 분류 문제를 해결해 보자.</br>\n",
    "입력값이 두개인 gate에 한해, A와 B의 값을 좌표평면에 나타내었을 때, output 값을 하나의 직선으로 구분할 수 있다면 linear gate라 하고,\n",
    "불가능하면 non-linear gate라 한다.</br>\n",
    "아래 그림은 AND gate의 Truth table과 좌표평면에 나타낸 그래프이다.(빨간색 숫자는 output(Q)).</br>\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fdff0028b-0962-4eb9-af66-2b5007d6c577%2FUntitled.png?table=block&id=b7ab01fc-64d7-44b0-8c50-fd5f22be63c0&spaceId=a014167e-a523-47c8-a378-515f190bb9a9&width=2000&userId=60188a97-735e-40f9-b614-fbec370125ac&cache=v2)\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fd3c06ed5-d39b-4f34-9450-876119fad27d%2FUntitled.png?table=block&id=07cec40d-084d-4627-bcdc-21c349eedfe7&spaceId=a014167e-a523-47c8-a378-515f190bb9a9&width=2000&userId=60188a97-735e-40f9-b614-fbec370125ac&cache=v2)\n",
    "\n",
    "> 이처럼 하나의 (초록색)직선으로 gate의 output을 구분할 수 있는 linear gate에는 AND,OR,NAND,NOR gate가 있다. </br>\n",
    "> 나머지는 직접 그려보도록 하자.\n",
    "\n",
    "</br>\n",
    "아래 그래프는 XOR gate의 input과 output을 좌표평면에 나타낸것이다.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F19602cdc-64e6-4326-92ee-42bff524bc58%2FUntitled.png?table=block&id=411cae73-ed74-4570-8d10-29e2b4e70497&spaceId=a014167e-a523-47c8-a378-515f190bb9a9&width=2000&userId=60188a97-735e-40f9-b614-fbec370125ac&cache=v2)\n",
    "\n",
    "</br>\n",
    "\n",
    "> linear gate와는 다르게 하나의 직선으로 0과 1을 구분지을 수 없다. 이 문제에 대한 해결방법을 생각 해 보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95caad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T13:03:27.782548Z",
     "start_time": "2022-02-10T13:03:27.766920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-<function AND_Perceptron at 0x00000219FF9F5280>-\n",
      "[0 0] -----> 0\n",
      "[0 1] -----> 0\n",
      "[1 0] -----> 0\n",
      "[1 1] -----> 1\n",
      "----------------\n",
      "-<function OR_Perceptron at 0x00000219FF9F5550>-\n",
      "[0 0] -----> 0\n",
      "[0 1] -----> 1\n",
      "[1 0] -----> 1\n",
      "[1 1] -----> 1\n",
      "----------------\n",
      "-<function NAND_Perceptron at 0x00000219FF9F54C0>-\n",
      "[0 0] -----> 1\n",
      "[0 1] -----> 1\n",
      "[1 0] -----> 1\n",
      "[1 1] -----> 0\n",
      "----------------\n",
      "-<function XOR_Perceptron at 0x00000219FF9F5CA0>-\n",
      "[0 0] -----> 0\n",
      "[0 1] -----> 1\n",
      "[1 0] -----> 1\n",
      "[1 1] -----> 0\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def AND_Perceptron(x1,x2):\n",
    "    w = np.array([0.5,0.5])\n",
    "    b = -0.7 \n",
    "    theta = 0\n",
    "\n",
    "    x = np.array([x1,x2])\n",
    "    y = np.sum(w*x)+b\n",
    "\n",
    "    if y > theta:\n",
    "        return 1\n",
    "    elif y <= theta:\n",
    "        return 0\n",
    "\n",
    "def OR_Perceptron(x1,x2):\n",
    "    w = np.array([0.5,0.5])\n",
    "    b = 0\n",
    "    theta = 0\n",
    "\n",
    "    x = np.array([x1,x2])\n",
    "    y = np.sum(w*x)+b\n",
    "    \n",
    "    if y > theta:\n",
    "        return 1\n",
    "    elif y <= theta:\n",
    "        return 0\n",
    "\n",
    "def NAND_Perceptron(x1,x2):\n",
    "    w = np.array([0.5,0.5])\n",
    "    b = 0\n",
    "    theta = 0.5\n",
    "\n",
    "    x = np.array([x1,x2])\n",
    "    y = np.sum(w*x)+b\n",
    "    \n",
    "    if y <= theta:\n",
    "        return 1\n",
    "    elif y > theta:\n",
    "        return 0   \n",
    "\n",
    "def XOR_Perceptron(x1,x2):\n",
    "    l1_0 = OR_Perceptron(x1,x2)\n",
    "    l1_1 = NAND_Perceptron(x1,x2)\n",
    "    l2   = AND_Perceptron(l1_0,l1_1)\n",
    "    return l2\n",
    "\n",
    "\n",
    "inputData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Perceptrons = [AND_Perceptron,OR_Perceptron,NAND_Perceptron,XOR_Perceptron]\n",
    "for box in Perceptrons:\n",
    "    print(f\"-{box}-\")\n",
    "    for i in inputData:\n",
    "        print(str(i) + \" -----> \" + str(box(i[0],i[1])))\n",
    "    print(\"-\"*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ac831",
   "metadata": {},
   "source": [
    "###  Affine function\n",
    "\n",
    "**Affine transformation, 인공 신경망의 forward propagation 과정에서 수행하는 행렬의 곱**\n",
    "\n",
    "![e.PNG](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F71bb67cc-dd56-44c7-ab65-87270e5f7476%2FUntitled.png?table=block&id=d861448f-ad93-405f-b182-09163463593d&spaceId=25594023-16cf-4fa0-8dc8-2bdd296293dc&width=2000&userId=3e4a911a-b5d6-4817-ab19-e1fd92acebe9&cache=v2)\n",
    "\n",
    "Affine layer에서 일어나는 forward propagation은 단순한 연산이니 backward propagation에 대해서 알아보자\n",
    "- 위 모식도에서 나온 결과값 Z가 어떤식으로 미분이 되며 backpropagation이 진행되는지 그 과정을 살펴보자\n",
    "\n",
    "- 아래 식은 일반적인 matrix multiplication을 나타낸 것이다.</br>\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa46d1a92-998e-4b80-96d3-9e6d9ea2e343%2FUntitled.png?table=block&id=1cb5e90d-5988-49bf-b175-22f290bcb7c2&spaceId=25594023-16cf-4fa0-8dc8-2bdd296293dc&width=1960&userId=3e4a911a-b5d6-4817-ab19-e1fd92acebe9&cache=v2)\n",
    "\n",
    "- 위 식에서 $a_i2$ 결과값은 i행에만 영향을 미친다는 사실을 기억하자\n",
    "- 이로 인해 미분연산 시 나머지는 상수항으로 취급되어 아래와 같은 방식으로 전미분을 통해 간단한 결과값을 얻을 수 있다.</br>\n",
    "$\\frac{\\partial J}{\\partial a_{ij}} =\\frac{\\partial J}{\\partial c_{i1}} \\frac{\\partial c_{i1}}{\\partial a_{ij}} + \n",
    "\\frac{\\partial J}{\\partial c_{i2}} \\frac{\\partial c_{i2}}{\\partial a_{ij}} + \n",
    "\\dots  +\n",
    "\\frac{\\partial J}{\\partial c_{i \\gamma}} \\frac{\\partial c_{i\\gamma}}{\\partial a_{ij}}=dc_{i1} \\cdot b_{j1} + \n",
    "dc_{i2} \\cdot b_{j2} + \n",
    "\\dots  +\n",
    "dc_{i\\gamma } \\cdot b_{j\\gamma }=dc_{i1} \\cdot (b_{T})_{1j}+ \n",
    "dc_{i2} \\cdot (b_{T})_{2j} + \n",
    "\\dots  +\n",
    "dc_{i\\gamma } \\cdot (b_{T})_{\\gamma j}$\n",
    "</br>\n",
    "$=Row_{i}(dC) \\cdot Col_{j}(B^{T})$\n",
    "\n",
    "> $J$ 는 activation함수를 통과한 최종값이다.\n",
    "\n",
    " $dX = dZ \\cdot W^{T}$\n",
    "\n",
    "> X는 input값 \n",
    "\n",
    "**※ Python을 통한 구현** </br>\n",
    "```python\n",
    "class Affine:\n",
    "\n",
    "    def __init__(self, W, B):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.B\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf87b2e",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "- 활성화 함수(Activation Function)는 신경학적으로 볼때 뉴런발사(Firing of a Neuron)의 과정에 해당한다고 볼 수 있다.</br> \n",
    "\n",
    "- 최종출력 신호를 다음 뉴런으로 보내줄지 말지 결정하는 역할을 하게 된다.</br>\n",
    "\n",
    "- 뉴런이 다음 뉴런으로 신호를 보낼 때 입력신호가 일정 기준 이상이면 보내고 기준에 달하지 못하면 보내지 않을 수도 있다. 그 신호를 결정 해주는 것이 활성화 함수(Activation Function)라 이해하면 된다. 많은 종류의 활성화 함수가 있고, Activation function의 결정이 결과에 크게 영향을 준다.\n",
    "\n",
    " **※ Linear function**</br>\n",
    " **선형함수는 말 그대로 직선적인 함수(y=x)이다.**\n",
    "![e.PNG](https://t1.daumcdn.net/cfile/tistory/993FAA4B5B6CF9BC03)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> 위의 수식을 이해 해 보면 Linear Function을 활성화 함수로 하게되었을 때 layer가 \"Deep\"한 네트워크의 이점이 전혀 없다는 것을 알 수 있다.\n",
    "\n",
    "</br></br></br>\n",
    "**※ Step Function**</br>\n",
    "**입력이 양수일때는 1(보낸다)을 음수일때는 0(보내지 않는다)의 신호를 보내주는 이진적인(Binary) 함수이다.**\n",
    "![e.PNG](https://t1.daumcdn.net/cfile/tistory/99F0DB455B6D2C2315)\n",
    "\n",
    "**※ Sigmoid Function** </br>\n",
    "**입력을 (0,1) 사이의 값으로 normalize해준다.** </br></br>\n",
    "+ 단점</br></br>\n",
    "\n",
    "1) gradient가 소실되는 현상이 발생한다 (Gradient vanishing 문제)</br>\n",
    "</br>\n",
    "    gradient 0이 곱해 지니까 그다음 layer로 전파되지 않는다. 즉, 학습이 되지 않는다.</br>\n",
    "</br>\n",
    "2) 활성함수의 결과 값의 중심이 0이 아닌 0.5이다.</br>\n",
    "</br>\n",
    "3) 계산이 복잡하다 (지수함수 계산)</br></br></br></br>\n",
    "**$sigmoid(x)=\\frac { 1 }{ 1+{ e }^{ -x } }$** </br></br>\n",
    "**$\\frac { d }{ \\partial x } sigmoid(x)=\\frac { 1 }{ 1+{ e }^{ -x } } \\left( 1-\\frac { 1 }{ 1+{ e }^{ -x } }  \\right)$**\n",
    "\n",
    "![e.PNG](https://t1.daumcdn.net/cfile/tistory/994E183D5B6D2C230A)\n",
    "</br></br></br>\n",
    "\n",
    "**※ ReLU Function (Rectified Linear Unit)** </br>\n",
    "<!-- **Relu(x)=max(0,x)** </br> -->\n",
    "**$Relu(x) = max(0,x)$**\n",
    "![e.PNG](https://t1.daumcdn.net/cfile/tistory/990E4E3F5B6D2CAE28)\n",
    "</br>\n",
    "+ 장점</br></br>\n",
    "(1) 양 극단값이 포화되지 않는다. (양수 지역은 선형적)</br>\n",
    "</br>\n",
    "(2) 계산이 매우 효율적이다 (최대값 연산 1개)</br>\n",
    "</br>\n",
    "(3) 수렴속도가 시그모이드류 함수대비 6배 정도 빠르다.</br>\n",
    "</br>\n",
    "+ 단점</br>\n",
    "</br>\n",
    "(1) 중심값이 0이 아님 (마이너한 문제)</br>\n",
    "</br>\n",
    "(2) 입력값이 음수인 경우 항상 0을 출력함 (마찬가지로 파라미터 업데이트가 안됨)</br>\n",
    "</br></br>\n",
    "\n",
    "**※ Python을 통한 구현** \n",
    "\n",
    "``` python\n",
    "\n",
    "class sigmoid:\n",
    "    def __init__(self):\n",
    "        self.bool = None \n",
    "        \n",
    "    ##순전파 과정    \n",
    "    def forward(self,x):\n",
    "        j = 1/(1 + np.exp(-x))\n",
    "        self.j = j \n",
    "        return j\n",
    "    \n",
    "    ##역전파 과정 \n",
    "    def backward(self,diff_)\n",
    "        dj = diff_ * self.out * (1.0-self.out)\n",
    "        return dj\n",
    "\n",
    "class relu:\n",
    "    def __init__(self):\n",
    "        self.bool = None\n",
    "\n",
    "    ##순전파 과정    \n",
    "    def forward(self,x):\n",
    "        self.bool = (x<=0)\n",
    "        j = x.copy()       ## copy() , 얕은 복사\n",
    "        j[self.bool] = 0\n",
    "        return j\n",
    "    \n",
    "    ##역전파 과정     \n",
    "    def backward(self,d_j):\n",
    "        diff_j[self.bool] = 0\n",
    "        dj = diff_j\n",
    "        return dj\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f5f63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T06:17:17.184452Z",
     "start_time": "2022-02-10T06:17:17.168454Z"
    }
   },
   "source": [
    "### Training</br>\n",
    "\n",
    " - 퍼셉트론은 네트워크가 예측할 때 Loss function을 통해 만드는 오차를 반영하도록 하여 오차가 감소하는 방향으로 연결을 강화시킨다.\n",
    " \n",
    " - 즉, 모든 출력 뉴런에 대해 올바른 예측을 만들 수 있도록 아래 수식과 같이 입력에 연결된 가중치를 강화시킨다.\n",
    " \n",
    ">[  $w_i,j^{(next step)} = w_i,j + n(y_j - \\hat{y}_j)x_i$ ]\n",
    "> \n",
    "> $w_i,j$는 i번째 입력뉴련과j번째 뉴런의 입력값\n",
    ">\n",
    "> $x_i$는 현재 훈련 샘플의 i번째 뉴런의 입력값\n",
    ">\n",
    "> $\\hat{y}_i$는 현재 훈련 샘플의 j번째 출력 뉴런의 출력값\n",
    ">\n",
    "> $y_j$는 현재 훈련 샘플의 j번째 출력 뉴런의 타깃값\n",
    ">\n",
    "> $n$ = 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff7a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
